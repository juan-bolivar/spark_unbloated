---
- name: spark installation
  hosts: all
  tasks:

  - name: Creating user 
    user:
      name:  ansible
      shell: /bin/bash
      groups: sudo 
      append: yes
      state: present
      create_home: True 

  - name: SSH KeyGen command
    tags: run
    shell: > 
      ssh-keygen -q -b 2048 -t rsa -N "" -f ~/.ssh/id_rsa
      creates="~/.ssh/id_rsa"

  - name: Fetch the keyfile from the node to master
    tags: run
    fetch: 
      src: "~/.ssh/id_rsa.pub"
      dest: "buffer/{{ ansible_hostname }}-id_rsa.pub"
      flat: yes

  - name: Copy the key add to authorized_keys using Ansible module
    tags: runcd
    authorized_key:
      user: root 
      state: present
      key: "{{ lookup('file','buffer/{{item}}-id_rsa.pub')}}"
    when: "{{ item != ansible_hostname }}"
    with_items: 
      - "{{ groups['all'] }}"      

  - name: installing java
    package: 
      name: openjdk-8-jre-headless
      state: present
      update_cache: yes 
  - name: installing scala
    package:
      name: scala
      state: present

  - name: install unzip
    package:
      name: unzip
      state: present

  - name: downloading spark
    get_url: 
      url:  'https://downloads.apache.org/spark/spark-3.1.1/spark-3.1.1-bin-hadoop2.7.tgz'
      dest: ~/spark.tgz
      force: yes

  - name: creating archive
    file:
      path: /usr/local/spark
      state: directory
      force: yes
  - name: unarchive spark
    command: 'tar xvf ~/spark.tgz'
    tags: unarchive

  - name: moving files
    copy: 
      remote_src: True
      src: spark-3.1.1-bin-hadoop2.7
      dest: /usr/local/
  - name: profile spark
    command: "echo 'export PATH=/usr/local/spark/bin:$PATH' >> ~/.profile ; source ~/.profile"

  - name: copying configuring files
    copy:
      src: /usr/local/spark-3.1.1-bin-hadoop2.7/conf/spark-env.sh.template
      dest: /usr/local/spark-3.1.1-bin-hadoop2.7/conf/spark-env.sh
      remote_src: yes


  - name: editing config file
    lineinfile:
      path: /usr/local/spark-3.1.1-bin-hadoop2.7/conf/spark-env.sh
      line: 'export SPARK_MASTER_HOST={{ hostvars[item]["ansible_facts"]["default_ipv4"]["address"] }}'
    with_items: 
      - "{{ groups['master'] }}"      


  - name: editing config file
    lineinfile:
      path: /usr/local/spark-3.1.1-bin-hadoop2.7/conf/spark-env.sh
      line: 'export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64/jre' 

        # - name: editing config file
        #blockinfile: 
        #dest: /usr/local/spark-3.1.1-bin-hadoop2.7/conf/workers
        #content: "
        #        165.227.118.177
        #       159.89.188.151
        #       159.89.42.27
        #       159.89.184.90
        #       " 


  - name: editing bashrc 
    lineinfile:
      path: ~/.bashrc 
      line: 'export PATH=$PATH:/usr/local/spark-3.1.1-bin-hadoop2.7/bin' 

        #  - name: refresh bashrc
        #command: 'source ~/.bashrc'
        #
  - name: copying workers file 
    copy:
      src: /usr/local/spark-3.1.1-bin-hadoop2.7/conf/workers.template
      dest: /usr/local/spark-3.1.1-bin-hadoop2.7/conf/workers
      remote_src: yes


  - name: adding workers 
    tags: runcd
    lineinfile:
      path: /usr/local/spark-3.1.1-bin-hadoop2.7/conf/workers
      line: '{{ hostvars[item]["ansible_facts"]["default_ipv4"]["address"] }} ' 
    with_items: 
      - "{{ groups['multi'] }}"      

  - name: getting pip 
    tags: pip
    command: 'curl "https://bootstrap.pypa.io/get-pip.py" -o get-pip.py' 

  - name: installing  pip
    tags: pip
    command: 'python3 get-pip.py' 

  - name: installing requirements 
    tags: pip
    command: 'python3 -m pip install koalas pandas pyspark findspark'

    
    
